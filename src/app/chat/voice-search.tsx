// voice-search.tsx
// ============================================
// ğŸ¤ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
// ============================================

"use client";

import React, { useState, useRef, useEffect } from 'react';
import { Mic, MicOff, Volume2, VolumeX, Loader2 } from 'lucide-react';

interface VoiceSearchProps {
  onSearchQuery: (query: string) => void;
  onError: (error: string) => void;
  language?: 'ar-SA' | 'en-US';
  disabled?: boolean;
}

interface SpeechRecognitionResult {
  transcript: string;
  confidence: number;
  isFinal: boolean;
}

export const VoiceSearch: React.FC<VoiceSearchProps> = ({
  onSearchQuery,
  onError,
  language = 'ar-SA',
  disabled = false
}) => {
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isSupported, setIsSupported] = useState(false);
  const [volume, setVolume] = useState(0);
  const [canSpeak, setCanSpeak] = useState(false);

  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationRef = useRef<number>();

  useEffect(() => {
    // Check for speech recognition support
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    const speechSynthesis = window.speechSynthesis;
    
    if (SpeechRecognition) {
      setIsSupported(true);
      recognitionRef.current = new SpeechRecognition();
      setupRecognition();
    } else {
      setIsSupported(false);
      onError('ğŸ¤ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØµÙØ­');
    }

    if (speechSynthesis) {
      setCanSpeak(true);
    }

    return () => {
      stopListening();
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, []);

  const setupRecognition = () => {
    if (!recognitionRef.current) return;

    const recognition = recognitionRef.current;
    
    recognition.continuous = false; // ØªØºÙŠÙŠØ± Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ£Ø®ÙŠØ±
    recognition.interimResults = true;
    recognition.lang = language;
    recognition.maxAlternatives = 1;

    recognition.onstart = () => {
      console.log('ğŸ¤ Voice recognition started');
      setIsListening(true);
      setIsProcessing(false);
      startVolumeMonitoring();
    };

    recognition.onresult = (event: any) => {
      let finalTranscript = '';
      let interimTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        const transcript = result[0].transcript;

        if (result.isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      const fullTranscript = finalTranscript || interimTranscript;
      setTranscript(fullTranscript);

      // Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ù„Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
      if (finalTranscript.trim() && finalTranscript.length > 2) {
        console.log('ğŸ¯ Final transcript:', finalTranscript);
        handleVoiceResult(finalTranscript.trim());
        recognition.stop(); // Ø¥ÙŠÙ‚Ø§Ù ÙÙˆØ±ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ£Ø®ÙŠØ±
      }
    };

    recognition.onerror = (event: any) => {
      console.error('âŒ Voice recognition error:', event.error);
      
      let errorMessage = 'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ';
      switch (event.error) {
        case 'no-speech':
          errorMessage = 'ğŸ”‡ Ù„Ù… ÙŠØªÙ… Ø±ØµØ¯ Ø£ÙŠ ØµÙˆØªØŒ ØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ¨Ø±';
          break;
        case 'audio-capture':
          errorMessage = 'ğŸ¤ ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø°ÙˆÙ†Ø§Øª';
          break;
        case 'not-allowed':
          errorMessage = 'â›” ØªÙ… Ø±ÙØ¶ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„';
          break;
        case 'network':
          errorMessage = 'ğŸŒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª';
          break;
        default:
          errorMessage = `âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ: ${event.error}`;
      }
      
      onError(errorMessage);
      stopListening();
    };

    recognition.onend = () => {
      console.log('ğŸ Voice recognition ended');
      setIsListening(false);
      setIsProcessing(false);
      setTranscript('');
      stopVolumeMonitoring();
    };
  };

  const startVolumeMonitoring = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioContextRef.current = new AudioContext();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      
      analyserRef.current.fftSize = 256;
      source.connect(analyserRef.current);
      
      const updateVolume = () => {
        if (!analyserRef.current) return;
        
        const bufferLength = analyserRef.current.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyserRef.current.getByteFrequencyData(dataArray);
        
        const average = dataArray.reduce((a, b) => a + b) / bufferLength;
        setVolume(average / 255);
        
        if (isListening) {
          animationRef.current = requestAnimationFrame(updateVolume);
        }
      };
      
      updateVolume();
    } catch (error) {
      console.warn('Volume monitoring failed:', error);
    }
  };

  const stopVolumeMonitoring = () => {
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
    }
    setVolume(0);
  };

  const handleVoiceResult = (transcript: string) => {
    // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©
    const cleanTranscript = transcript.trim().replace(/[.!?]+$/, '').replace(/\s+/g, ' ');
    
    console.log('ğŸš€ Instant voice processing:', cleanTranscript);
    
    // Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ£Ø®ÙŠØ±
    setIsProcessing(true);
    onSearchQuery(cleanTranscript);
    
    // Ø¥ÙŠÙ‚Ø§Ù ÙÙˆØ±ÙŠ
    stopListening();
    
    // ØªØ£ÙƒÙŠØ¯ ØµÙˆØªÙŠ Ø§Ø®ØªÙŠØ§Ø±ÙŠ
    if (canSpeak) {
      setTimeout(() => speakResponse(`Ø¨Ø­Ø«: ${cleanTranscript}`), 100);
    }
  };

  const startListening = () => {
    if (!isSupported || !recognitionRef.current || disabled) return;
    
    try {
      setTranscript('');
      setIsProcessing(false);
      recognitionRef.current.start();
    } catch (error) {
      console.error('Failed to start recognition:', error);
      onError('ğŸ¤ ÙØ´Ù„ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ');
    }
  };

  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
    }
    setIsListening(false);
    setIsProcessing(false);
    setTranscript('');
    stopVolumeMonitoring();
  };

  const speakResponse = (text: string) => {
    if (!canSpeak || !window.speechSynthesis) return;
    
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = language;
    utterance.rate = 0.9;
    utterance.pitch = 1;
    utterance.volume = 0.8;
    
    window.speechSynthesis.speak(utterance);
  };

  const toggleListening = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  if (!isSupported) {
    return (
      <div className="flex items-center gap-2 px-3 py-2 bg-gray-100 dark:bg-gray-800 rounded-lg text-sm text-gray-600 dark:text-gray-400">
        <MicOff className="w-4 h-4" />
        <span>Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…</span>
      </div>
    );
  }

  return (
    <div className="flex items-center gap-2">
      {/* Voice Search Button */}
      <button
        onClick={toggleListening}
        disabled={disabled || isProcessing}
        className={`
          relative p-3 rounded-full transition-all duration-300 flex items-center justify-center
          ${isListening 
            ? 'bg-red-500 hover:bg-red-600 text-white shadow-lg scale-110' 
            : 'bg-blue-500 hover:bg-blue-600 text-white shadow-md hover:shadow-lg'
          }
          ${disabled || isProcessing ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'}
          ${isListening ? 'animate-pulse' : ''}
        `}
        title={isListening ? 'Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ' : 'Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØµÙˆØªÙŠ'}
      >
        {isProcessing ? (
          <Loader2 className="w-5 h-5 animate-spin" />
        ) : isListening ? (
          <MicOff className="w-5 h-5" />
        ) : (
          <Mic className="w-5 h-5" />
        )}
        
        {/* Volume indicator */}
        {isListening && (
          <div 
            className="absolute -top-1 -right-1 w-3 h-3 bg-green-400 rounded-full"
            style={{ 
              opacity: Math.min(volume * 3, 1),
              transform: `scale(${1 + volume})` 
            }}
          />
        )}
      </button>

      {/* Live Transcript Display */}
      {(isListening || isProcessing) && (
        <div className="flex-1 min-w-0">
          <div className="bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-lg p-3 shadow-md">
            <div className="flex items-center gap-2 mb-1">
              <div className={`w-2 h-2 rounded-full ${isListening ? 'bg-red-500 animate-pulse' : 'bg-gray-400'}`} />
              <span className="text-xs font-medium text-gray-600 dark:text-gray-400">
                {isProcessing ? 'âš¡ Ø¨Ø­Ø« ÙÙˆØ±ÙŠ Ø¬Ø§Ø±ÙŠ...' : isListening ? 'ğŸ¤ Ø§Ø³ØªÙ…Ø¹...' : 'â¹ï¸ ØªÙˆÙ‚Ù'}
              </span>
            </div>
            <p className="text-sm text-gray-800 dark:text-gray-200 min-h-[20px]">
              {transcript || (isListening ? 'ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù†...' : 'Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...')}
            </p>
          </div>
        </div>
      )}

      {/* Speaker Toggle */}
      {canSpeak && (
        <button
          onClick={() => setCanSpeak(!canSpeak)}
          className="p-2 text-gray-600 dark:text-gray-400 hover:text-gray-800 dark:hover:text-gray-200 transition-colors"
          title={canSpeak ? 'ÙƒØªÙ… Ø§Ù„ØµÙˆØª' : 'ØªÙØ¹ÙŠÙ„ Ø§Ù„ØµÙˆØª'}
        >
          {canSpeak ? <Volume2 className="w-4 h-4" /> : <VolumeX className="w-4 h-4" />}
        </button>
      )}
    </div>
  );
};

export default VoiceSearch;